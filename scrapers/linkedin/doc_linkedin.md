# Documentaci√≥n - Scraper LinkedIn Jobs Argentina

## üìã √çndice
- [Descripci√≥n General](#descripci√≥n-general)
- [Arquitectura del Sistema](#arquitectura-del-sistema)
- [Estructura de Archivos](#estructura-de-archivos)
- [Componentes Principales](#componentes-principales)
- [Gu√≠a de Uso](#gu√≠a-de-uso)
- [API Reference](#api-reference)
- [Configuraci√≥n](#configuraci√≥n)
- [Ejemplos](#ejemplos)
- [Problemas comunes](#troubleshooting)

## üìñ Descripci√≥n General

El **Scraper LinkedIn Jobs** es un sistema automatizado de extracci√≥n de empleos optimizado para LinkedIn Argentina. Utiliza Playwright para navegaci√≥n web robusta y t√©cnicas anti-detecci√≥n para realizar scraping eficiente y confiable de ofertas laborales.

### Caracter√≠sticas Principales
- ‚úÖ **Scraping As√≠ncrono**: Procesamiento paralelo para m√°ximo rendimiento
- ‚úÖ **Anti-Detecci√≥n**: T√©cnicas stealth para evitar bloqueos
- ‚úÖ **Extracci√≥n Robusta**: Parser inteligente con m√∫ltiples selectores de respaldo
- ‚úÖ **Exportaci√≥n CSV**: Datos estructurados listos para an√°lisis
- ‚úÖ **Logging Avanzado**: Monitoreo completo del proceso
- ‚úÖ **Manejo de Errores**: Recuperaci√≥n autom√°tica y reintentos
- ‚úÖ **Login Autom√°tico**: Soporte para autenticaci√≥n opcional

## üèóÔ∏è Arquitectura del Sistema

```
scrapers/linkedin/
‚îú‚îÄ‚îÄ main.py          # Punto de entrada y orquestador principal
‚îú‚îÄ‚îÄ scraper.py       # Motor de scraping con Playwright
‚îú‚îÄ‚îÄ parser.py        # Extractor y parseador de datos HTML
‚îî‚îÄ‚îÄ __init__.py      # Inicializaci√≥n del m√≥dulo
```

### Flujo de Datos
```mermaid
graph TD
    A[main.py] --> B[LinkedInScraper]
    B --> C[LinkedInJobsScraper]
    C --> D[BrowserManager]
    C --> E[LinkedInParser]
    E --> F[JobData]
    F --> G[CSVExporter]
    G --> H[Archivo CSV]
```

## üìÅ Estructura de Archivos

### `/scrapers/linkedin/` - M√≥dulo Principal
```
linkedin/
‚îú‚îÄ‚îÄ __init__.py      # Exportaciones del m√≥dulo
‚îú‚îÄ‚îÄ main.py          # CLI y orquestador principal
‚îú‚îÄ‚îÄ scraper.py       # Motor de scraping
‚îî‚îÄ‚îÄ parser.py        # Parser de datos HTML
```

## üîß Componentes Principales

### 1. **main.py** - Orquestador Principal
```python
# Componentes principales
class CSVExporter          # Exportador de datos a CSV
class LinkedInScraper      # Gestor de sesi√≥n de scraping
async def main()           # Funci√≥n principal CLI
```

**Responsabilidades:**
- Gesti√≥n de argumentos CLI
- Coordinaci√≥n de componentes
- Exportaci√≥n de resultados
- Manejo de errores globales

### 2. **scraper.py** - Motor de Scraping
```python
class LinkedInJobsScraper:
    async def scrape_jobs()               # Scraping principal
    async def scrape_job_details()        # Scraping detallado
    async def _scrape_search_term()       # Scraping por t√©rmino
    async def _handle_access_and_login()  # Manejo de autenticaci√≥n
```

**Caracter√≠sticas:**
- Scraping as√≠ncrono con Playwright
- T√©cnicas anti-detecci√≥n
- Login autom√°tico opcional
- Manejo autom√°tico de reintentos
- Eliminaci√≥n de duplicados

### 3. **parser.py** - Extractor de Datos
```python
@dataclass
class JobData               # Estructura de datos de empleo

class LinkedInParser:
    async def find_job_elements()      # B√∫squeda de elementos
    async def parse_job_element()      # Parse de elemento individual
    async def scrape_job_details()     # Scraping de detalles
```

**Funcionalidades:**
- Selectores m√∫ltiples para robustez
- Validaci√≥n de datos extra√≠dos
- Limpieza autom√°tica de texto
- Extracci√≥n de metadatos de empleos

## üöÄ Gu√≠a de Uso

### Uso B√°sico
```bash
# Scraping con t√©rminos por defecto
python -m scrapers.linkedin.main

# T√©rminos espec√≠ficos
python -m scrapers.linkedin.main --terms "python developer" "data analyst"

# Limitar empleos
python -m scrapers.linkedin.main --max-jobs 20

# Modo de prueba
python -m scrapers.linkedin.main --test
```

### Par√°metros CLI
```bash
# Ayuda completa
python -m scrapers.linkedin.main --help

# Ejemplos de uso
python -m scrapers.linkedin.main --terms "frontend developer" --max-jobs 30
python -m scrapers.linkedin.main --test  # M√°ximo 10 empleos
```

## üìö API Reference

### JobData
Estructura de datos para empleos extra√≠dos.

```python
@dataclass
class JobData:
    indice: int                         # √çndice del empleo
    fecha_extraccion: str               # Fecha de extracci√≥n
    titulo_puesto: str                  # T√≠tulo del puesto
    empresa: str                        # Nombre de la empresa
    ubicacion: str                      # Ubicaci√≥n del empleo
    url_empleo: str                     # URL del empleo
    modalidad: str                      # Modalidad (Remoto/H√≠brido/Presencial)
    fecha_publicacion: str              # Fecha de publicaci√≥n
    descripcion_breve: str              # Descripci√≥n breve
    nivel_experiencia: str              # Nivel de experiencia requerido
    beneficios_ofrecidos: str           # Beneficios ofrecidos
```

### LinkedInJobsScraper

#### M√©todos Principales

**`scrape_jobs(search_terms: List[str], max_jobs: int) -> List[JobData]`**
- Scraping principal de empleos por t√©rminos de b√∫squeda
- Retorna lista de empleos √∫nicos
- Maneja m√∫ltiples t√©rminos de forma as√≠ncrona

**`scrape_job_details(jobs: List[JobData], max_details: int) -> List[JobData]`**
- Scraping detallado de empleos espec√≠ficos
- Extrae informaci√≥n adicional de p√°ginas individuales
- Optimizado para lotes de empleos

#### Configuraci√≥n

```python
scraper = LinkedInJobsScraper()
```

### LinkedInParser

#### M√©todos de Extracci√≥n

**`find_job_elements(page: Page, max_jobs: int) -> List[ElementHandle]`**
- Encuentra elementos de empleos en la p√°gina
- Usa m√∫ltiples selectores para robustez
- Valida elementos con contenido √∫til

**`parse_job_element(element: ElementHandle, index: int) -> Optional[JobData]`**
- Extrae datos de un elemento empleo
- Valida calidad de datos extra√≠dos
- Retorna None si datos insuficientes

**`scrape_job_details(page: Page, url: str) -> Dict[str, str]`**
- Extrae detalles adicionales de p√°gina individual
- Obtiene descripci√≥n completa y beneficios
- Maneja errores de navegaci√≥n

### CSVExporter

**`export_to_csv(jobs: List[JobData], filename: str = None) -> Optional[str]`**
- Exporta empleos a archivo CSV
- Genera nombre autom√°tico con timestamp
- Valida empleos antes de exportar
- Retorna ruta del archivo generado

### LinkedInScraper (Clase Principal)

**`run() -> Dict[str, Any]`**
- Ejecuta scraping completo
- Coordina todos los componentes
- Retorna estad√≠sticas de ejecuci√≥n

## ‚öôÔ∏è Configuraci√≥n

### Variables de Configuraci√≥n
```python
# En core/config.py
LINKEDIN_CONFIG = {
    'search_terms': [
        'python developer',
        'data analyst', 
        'frontend developer',
        'backend developer',
        'project manager'
    ],
    'email': 'tu_email@ejemplo.com',    # Opcional para login
    'password': 'tu_password',          # Opcional para login
    'max_jobs_per_term': 50,
    'timeout': 30000
}

SCRAPING_CONFIG = {
    'delay_range': (3, 8),
    'stealth_mode': True,
    'browser_args': ['--no-sandbox']
}
```

### Selectores CSS
Los selectores se configuran en `parser.py`:
```python
self.job_container_selectors = [
    'ul li:has(a[href*="/jobs/view/"])',
    'li[data-occludable-job-id]',
    'li:has(div div div div div div a[href*="/jobs/view/"])'
]

title_selectors = [
    'a[href*="/jobs/view/"] span strong',
    'a[href*="/jobs/view/"] strong',
    'div div div div div div a[href*="/jobs/view/"] span strong'
]
```

## üí° Ejemplos

### Ejemplo 1: Scraping B√°sico
```python
import asyncio
from scrapers.linkedin import LinkedInScraper

async def main():
    scraper = LinkedInScraper(
        search_terms=["python developer", "data analyst"],
        max_jobs=30
    )
    
    results = await scraper.run()
    
    if results['success']:
        print(f"Empleos extra√≠dos: {results['jobs_count']}")
        print(f"Archivo CSV: {results['csv_file']}")

asyncio.run(main())
```

### Ejemplo 2: Scraping con Detalles
```python
from scrapers.linkedin import scrape_linkedin_jobs

async def scrape_with_details():
    jobs = await scrape_linkedin_jobs(
        search_terms=["frontend developer"],
        max_jobs=20,
        include_details=True  # Incluir detalles completos
    )
    
    for job in jobs:
        print(f"T√≠tulo: {job.titulo_puesto}")
        print(f"Empresa: {job.empresa}")
        print(f"Modalidad: {job.modalidad}")
        print(f"Descripci√≥n: {job.descripcion_breve}")
        print("-" * 50)
```

### Ejemplo 3: An√°lisis de Datos
```python
from scrapers.linkedin import LinkedInJobsScraper

async def analyze_jobs():
    scraper = LinkedInJobsScraper()
    jobs = await scraper.scrape_jobs(["data scientist"], 50)
    
    # An√°lisis de modalidades
    modalidades = {}
    for job in jobs:
        modalidad = job.modalidad
        modalidades[modalidad] = modalidades.get(modalidad, 0) + 1
    
    print("Distribuci√≥n de modalidades:")
    for modalidad, count in modalidades.items():
        print(f"{modalidad}: {count}")
```

### Ejemplo 4: Exportaci√≥n Personalizada
```python
from scrapers.linkedin import CSVExporter, JobData

# Crear datos de ejemplo
jobs = [
    JobData(
        indice=1,
        titulo_puesto="Python Developer",
        empresa="TechCorp",
        ubicacion="Buenos Aires, Argentina",
        modalidad="Remoto"
    )
]

# Exportar
exporter = CSVExporter(output_dir="./exports")
csv_file = exporter.export_to_csv(jobs, "empleos_custom.csv")
```

## üîç Troubleshooting

### Problemas Comunes

#### 1. **No se encuentran empleos**
```
‚ùå No se encontraron empleos
```
**Soluciones:**
- Verificar t√©rminos de b√∫squeda
- Comprobar conectividad
- Revisar selectores CSS actualizados
- Verificar si LinkedIn requiere login

#### 2. **Errores de timeout**
```
TimeoutError: waiting for selector
```
**Soluciones:**
- Aumentar timeout en configuraci√≥n
- Verificar estabilidad de conexi√≥n
- Usar modo de prueba para debug
- Comprobar si LinkedIn est√° bloqueando el acceso

#### 3. **Problemas de login**
```
‚ùå Login fall√≥
```
**Soluciones:**
- Verificar credenciales en configuraci√≥n
- Usar login manual cuando se solicite
- Comprobar si LinkedIn requiere verificaci√≥n 2FA
- Verificar que la cuenta no est√© bloqueada

#### 4. **Posibles bloqueos**
```
‚ö†Ô∏è Posible bloqueo detectado
```
**Soluciones:**
- Reducir velocidad de scraping
- Usar diferentes user agents
- Implementar proxies rotativos
- Esperar tiempo entre sesiones

### Logs y Debugging

#### Activar Debug Logging
```python
from core.logger import LogConfig, get_logger

config = LogConfig(
    level="DEBUG",
    json_format=False
)
logger = get_logger("debug", config)
```

#### An√°lisis de Performance
```python
from core.logger import PerformanceLogger

perf_logger = PerformanceLogger(logger)
perf_logger.start("scraping_session")
# ... c√≥digo de scraping ...
perf_logger.end(success=True, jobs_count=25)
```

### Mejores Pr√°cticas

1. **Uso Responsable**
   - Respetar t√©rminos de servicio de LinkedIn
   - Implementar delays apropiados
   - No sobrecargar servidores
   - Usar datos solo para prop√≥sitos leg√≠timos

2. **Manejo de Autenticaci√≥n**
   - Usar credenciales propias √∫nicamente
   - No compartir informaci√≥n de login
   - Considerar usar API oficial cuando est√© disponible
   - Manejar verificaciones de seguridad

3. **Manejo de Errores**
   - Siempre usar try/catch
   - Implementar logging detallado
   - Manejar reconexiones
   - Validar datos extra√≠dos

4. **Optimizaci√≥n**
   - Limitar empleos por sesi√≥n
   - Usar scraping selectivo
   - Cachear resultados cuando sea posible
   - Monitorear recursos del sistema

5. **Mantenimiento**
   - Monitorear cambios en selectores
   - Actualizar user agents
   - Revisar logs regularmente
   - Mantener actualizadas las dependencias

### Campos CSV Exportados

El CSV exportado contiene los siguientes campos:

| Campo | Descripci√≥n | Ejemplo |
|-------|-------------|---------|
| `indice` | N√∫mero de √≠ndice del empleo | 1, 2, 3... |
| `fecha_extraccion` | Fecha cuando se extrajo | 2025-06-06 |
| `titulo_puesto` | T√≠tulo del puesto de trabajo | "Python Developer" |
| `empresa` | Nombre de la empresa | "TechCorp SA" |
| `ubicacion` | Ubicaci√≥n del empleo | "Buenos Aires, Argentina" |
| `url_empleo` | URL completa del empleo | "https://www.linkedin.com/jobs/view/123456" |
| `modalidad` | Tipo de trabajo | "Remoto", "H√≠brido", "Presencial" |
| `fecha_publicacion` | Cu√°ndo se public√≥ | "Hace 2 d√≠as" |
| `descripcion_breve` | Descripci√≥n corta del empleo | "Desarrollador Python con experiencia..." |
| `nivel_experiencia` | Nivel requerido | "Mid-Senior level" |
| `beneficios_ofrecidos` | Beneficios mencionados | "Seguro m√©dico, trabajo remoto" |

### Troubleshooting Avanzado

#### Error: "No se pudo acceder a LinkedIn"
```python
# Soluci√≥n: Verificar configuraci√≥n de red
import asyncio
from playwright.async_api import async_playwright

async def test_linkedin_access():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()
        
        try:
            await page.goto("https://www.linkedin.com/jobs/")
            print("‚úÖ Acceso exitoso a LinkedIn")
        except Exception as e:
            print(f"‚ùå Error de acceso: {e}")
        finally:
            await browser.close()

asyncio.run(test_linkedin_access())
```

#### Error: "Selectores no encontrados"
```python
# Soluci√≥n: Verificar selectores actuales
async def debug_selectors(page):
    selectors_to_test = [
        'ul li:has(a[href*="/jobs/view/"])',
        'li[data-occludable-job-id]',
        'a[href*="/jobs/view/"]'
    ]
    
    for selector in selectors_to_test:
        try:
            elements = await page.query_selector_all(selector)
            print(f"Selector '{selector}': {len(elements)} elementos")
        except Exception as e:
            print(f"Selector '{selector}': Error - {e}")
```
