# Documentaci√≥n - Scraper MercadoLibre Argentina

## üìã √çndice
- [Descripci√≥n General](#descripci√≥n-general)
- [Arquitectura del Sistema](#arquitectura-del-sistema)
- [Estructura de Archivos](#estructura-de-archivos)
- [Componentes Principales](#componentes-principales)
- [Gu√≠a de Uso](#gu√≠a-de-uso)
- [API Reference](#api-reference)
- [Configuraci√≥n](#configuraci√≥n)
- [Ejemplos](#ejemplos)
- [Problemas comunes](#Troubleshooting)


## üìñ Descripci√≥n General

El **Scraper MercadoLibre** es un sistema automatizado de extracci√≥n de datos optimizado para MercadoLibre Argentina. Utiliza Playwright para navegaci√≥n web robusta y t√©cnicas anti-detecci√≥n para realizar scraping eficiente y confiable.

### Caracter√≠sticas Principales
- ‚úÖ **Scraping As√≠ncrono**: Procesamiento paralelo para m√°ximo rendimiento
- ‚úÖ **Anti-Detecci√≥n**: T√©cnicas stealth para evitar bloqueos
- ‚úÖ **Extracci√≥n Robusta**: Parser inteligente con m√∫ltiples selectores de respaldo
- ‚úÖ **Exportaci√≥n CSV**: Datos estructurados listos para an√°lisis
- ‚úÖ **Logging Avanzado**: Monitoreo completo del proceso
- ‚úÖ **Manejo de Errores**: Recuperaci√≥n autom√°tica y reintentos

## üèóÔ∏è Arquitectura del Sistema

```
scrapers/mercadolibre/
‚îú‚îÄ‚îÄ main.py          # Punto de entrada y orquestador principal
‚îú‚îÄ‚îÄ scraper.py       # Motor de scraping con Playwright
‚îú‚îÄ‚îÄ parser.py        # Extractor y parseador de datos HTML
‚îî‚îÄ‚îÄ __init__.py      # Inicializaci√≥n del m√≥dulo
```

### Flujo de Datos
```mermaid
graph TD
    A[main.py] --> B[ScrapingSession]
    B --> C[MercadoLibreScraper]
    C --> D[BrowserManager]
    C --> E[MercadoLibreParser]
    E --> F[ProductData]
    F --> G[CSVExporter]
    G --> H[Archivo CSV]
```

## üìÅ Estructura de Archivos

### `/scrapers/mercadolibre/` - M√≥dulo Principal
```
mercadolibre/
‚îú‚îÄ‚îÄ __init__.py      # Exportaciones del m√≥dulo
‚îú‚îÄ‚îÄ main.py          # CLI y orquestador principal
‚îú‚îÄ‚îÄ scraper.py       # Motor de scraping
‚îî‚îÄ‚îÄ parser.py        # Parser de datos HTML
```

## üîß Componentes Principales

### 1. **main.py** - Orquestador Principal
```python
# Componentes principales
class CSVExporter          # Exportador de datos a CSV
class ScrapingSession      # Gestor de sesi√≥n de scraping
async def main()           # Funci√≥n principal CLI
```

**Responsabilidades:**
- Gesti√≥n de argumentos CLI
- Coordinaci√≥n de componentes
- Exportaci√≥n de resultados
- Manejo de errores globales

### 2. **scraper.py** - Motor de Scraping
```python
class MercadoLibreScraper:
    async def scrape_products()           # Scraping principal
    async def scrape_product_details()    # Scraping detallado
    async def _scrape_search_term()       # Scraping por t√©rmino
    async def _extract_products_with_parser()  # Extracci√≥n con parser
```

**Caracter√≠sticas:**
- Scraping as√≠ncrono con Playwright
- T√©cnicas anti-detecci√≥n
- Manejo autom√°tico de reintentos
- Eliminaci√≥n de duplicados

### 3. **parser.py** - Extractor de Datos
```python
@dataclass
class ProductData           # Estructura de datos de producto

class MercadoLibreParser:
    async def find_product_elements()     # B√∫squeda de elementos
    async def parse_product_element()     # Parse de elemento individual
    async def scrape_product_details()    # Scraping de detalles
```

**Funcionalidades:**
- Selectores m√∫ltiples para robustez
- Validaci√≥n de datos extra√≠dos
- Limpieza autom√°tica de texto
- Extracci√≥n de metadatos

## üöÄ Gu√≠a de Uso


### Uso B√°sico
```bash
# Scraping con t√©rminos por defecto
python -m scrapers.mercadolibre.main

# T√©rminos espec√≠ficos
python -m scrapers.mercadolibre.main --terms "notebook" "smartphone"

# Limitar productos
python -m scrapers.mercadolibre.main --max-products 20

# Modo de prueba
python -m scrapers.mercadolibre.main --test
```

## üìö API Reference

### ProductData
Estructura de datos para productos extra√≠dos.

```python
@dataclass
class ProductData:
    producto: str                    # Nombre del producto
    precio: str                      # Precio formateado
    vendedor: str                    # Nombre del vendedor
    ubicacion: str                   # Ubicaci√≥n del vendedor
    reputacion_vendedor: str         # Reputaci√≥n del vendedor
    fecha_extraccion: str            # Fecha de extracci√≥n
    url_producto: str                # URL del producto
    disponible: str                  # Disponibilidad (S√≠/No)
    envio_gratis: str                # Env√≠o gratis (S√≠/No)
    categoria: str                   # Categor√≠a/t√©rmino de b√∫squeda
    imagen_url: str                  # URL de imagen
    condicion: str                   # Condici√≥n del producto
```

### MercadoLibreScraper

#### M√©todos Principales

**`scrape_products(search_terms: List[str]) -> List[ProductData]`**
- Scraping principal de productos por t√©rminos de b√∫squeda
- Retorna lista de productos √∫nicos
- Maneja m√∫ltiples t√©rminos de forma as√≠ncrona

**`scrape_product_details(product_urls: List[str]) -> List[Dict[str, Any]]`**
- Scraping detallado de URLs espec√≠ficas
- Extrae informaci√≥n adicional de p√°ginas individuales
- Optimizado para lotes de productos

#### Configuraci√≥n

```python
scraper = MercadoLibreScraper()
scraper.max_products = 100  # M√°ximo productos por t√©rmino
```

### MercadoLibreParser

#### M√©todos de Extracci√≥n

**`find_product_elements(page: Page) -> List[ElementHandle]`**
- Encuentra elementos de productos en la p√°gina
- Usa m√∫ltiples selectores para robustez
- Valida elementos con contenido √∫til

**`parse_product_element(element: ElementHandle, search_term: str) -> Optional[ProductData]`**
- Extrae datos de un elemento producto
- Valida calidad de datos extra√≠dos
- Retorna None si datos insuficientes

**`scrape_product_details(page: Page, url: str) -> Dict[str, str]`**
- Extrae detalles adicionales de p√°gina individual
- Obtiene ubicaci√≥n y reputaci√≥n del vendedor
- Maneja errores de navegaci√≥n

### CSVExporter

**`export_to_csv(products: List[ProductData], filename: str = None) -> Optional[str]`**
- Exporta productos a archivo CSV
- Genera nombre autom√°tico con timestamp
- Valida productos antes de exportar
- Retorna ruta del archivo generado

## ‚öôÔ∏è Configuraci√≥n

### Variables de Configuraci√≥n
```python
# En core/config.py
MERCADOLIBRE_CONFIG = {
    'max_products_per_term': 50,
    'search_terms': ['zapatillas', 'notebook'],
    'timeout': 30000,
    'retry_attempts': 3
}

SCRAPING_CONFIG = {
    'delay_range': (1, 3),
    'stealth_mode': True,
    'browser_args': ['--no-sandbox']
}
```

### Selectores CSS
Los selectores se configuran en `parser.py`:
```python
self.selectors = {
    'containers': [
        ".ui-search-layout__item",
        ".ui-search-results__item"
    ],
    'titles': [
        ".poly-component__title",
        ".ui-search-item__title"
    ],
    'prices': [
        ".andes-money-amount__fraction"
    ]
}
```

## üí° Ejemplos

### Ejemplo 1: Scraping B√°sico
```python
import asyncio
from scrapers.mercadolibre import ScrapingSession

async def main():
    session = ScrapingSession(
        search_terms=["notebook", "smartphone"],
        max_products=30
    )
    
    results = await session.run_scraping()
    
    if results['success']:
        print(f"Productos extra√≠dos: {results['products_scraped']}")
        print(f"Archivo CSV: {results['csv_file']}")

asyncio.run(main())
```

### Ejemplo 2: Procesamiento de Datos
```python
from scrapers.mercadolibre import MercadoLibreScraper

async def analyze_products():
    scraper = MercadoLibreScraper()
    products = await scraper.scrape_products(["heladera"])
    
    # An√°lisis de precios
    prices = [p.precio for p in products if p.precio != "N/A"]
    avg_price = sum(int(p.replace('$', '').replace('.', '')) 
                   for p in prices) / len(prices)
    
    print(f"Precio promedio: ${avg_price:,.0f}")
```

### Ejemplo 3: Exportaci√≥n Personalizada
```python
from scrapers.mercadolibre import CSVExporter, ProductData

# Crear datos de ejemplo
products = [
    ProductData(
        producto="Notebook Dell",
        precio="$250.000",
        vendedor="TechStore",
        categoria="notebook"
    )
]

# Exportar
exporter = CSVExporter(output_dir="./exports")
csv_file = exporter.export_to_csv(products, "productos_custom.csv")
```

## üîç Troubleshooting

### Problemas Comunes

#### 1. **No se encuentran productos**
```
‚ùå No se encontraron productos
```
**Soluciones:**
- Verificar t√©rminos de b√∫squeda
- Comprobar conectividad
- Revisar selectores CSS actualizados

#### 2. **Errores de timeout**
```
TimeoutError: waiting for selector
```
**Soluciones:**
- Aumentar timeout en configuraci√≥n
- Verificar estabilidad de conexi√≥n
- Usar modo de prueba para debug

#### 3. **Posibles bloqueos**
```
‚ö†Ô∏è Posible bloqueo detectado
```
**Soluciones:**
- Reducir velocidad de scraping
- Usar diferentes user agents
- Implementar proxies rotativos

### Logs y Debugging

#### Activar Debug Logging
```python
from core.logger import LogConfig, get_logger

config = LogConfig(
    level="DEBUG",
    json_format=False
)
logger = get_logger("debug", config)
```

#### An√°lisis de Performance
```python
from core.logger import PerformanceLogger

perf_logger = PerformanceLogger(logger)
perf_logger.start("scraping_session")
# ... c√≥digo de scraping ...
perf_logger.end(success=True, products_count=50)
```

### Mejores Pr√°cticas

1. **Uso Responsable**
   - Respetar robots.txt
   - Implementar delays apropiados
   - No sobrecargar servidores

2. **Manejo de Errores**
   - Siempre usar try/catch
   - Implementar logging detallado
   - Manejar reconexiones

3. **Optimizaci√≥n**
   - Limitar productos por sesi√≥n
   - Usar scraping selectivo
   - Cachear resultados cuando sea posible

4. **Mantenimiento**
   - Monitorear cambios en selectores
   - Actualizar user agents
   - Revisar logs regularmente
